{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11e741bb-2db3-4507-a452-9f8b3bc1ac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b71b08-03f4-43ac-9410-5763b8b0d317",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bca1c7bc-7e25-47c1-b6ea-355d3cfcc020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# git clone https://github.com/google-deepmind/mujoco_menagerie.git\n",
    "\n",
    "# Basic loading (uncomment to use)\n",
    "# env = gym.make('Ant-v5', xml_file='./mujoco_menagerie/unitree_go1/scene.xml')\n",
    "\n",
    "# Although this is enough to load the model, we will need to tweak some environment parameters\n",
    "# to get the desired behavior for our environment, so we will also explicitly set the simulation,\n",
    "# termination, reward and observation arguments, which we will tweak in the next step.\n",
    "\n",
    "env = gym.make(\n",
    "    \"Ant-v5\",\n",
    "    xml_file=\"./unitree_go1/scene.xml\",\n",
    "    forward_reward_weight=0,\n",
    "    ctrl_cost_weight=0,\n",
    "    contact_cost_weight=0,\n",
    "    healthy_reward=0,\n",
    "    main_body=1,\n",
    "    healthy_z_range=(0, np.inf),\n",
    "    include_cfrc_ext_in_observation=True,\n",
    "    exclude_current_positions_from_observation=False,\n",
    "    reset_noise_scale=0,\n",
    "    frame_skip=1,\n",
    "    max_episode_steps=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2539c37f-0f26-4d2a-bb06-69e5d3a2f741",
   "metadata": {},
   "source": [
    "# Tweaking the Environment Simulation Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e82618bc-0b21-4796-b2dd-16ee0a238e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to tweak the `frame_skip` parameter to get `dt` to an acceptable value\n",
    "# (typical values are `dt` ∈ [0.01, 0.1] seconds),\n",
    "\n",
    "# Reminder: dt = frame_skip × model.opt.timestep, where `model.opt.timestep` is the integrator\n",
    "# time step selected in the MJCF model file.\n",
    "\n",
    "# The `Go1` model we are using has an integrator timestep of `0.002`, so by selecting\n",
    "# `frame_skip=25` we can set the value of `dt` to `0.05s`.\n",
    "\n",
    "# To avoid overfitting the policy, `reset_noise_scale` should be set to a value appropriate\n",
    "# to the size of the robot, we want the value to be as large as possible without the initial\n",
    "# distribution of states being invalid (`Terminal` regardless of control actions),\n",
    "# for `Go1` we choose a value of `0.1`.\n",
    "\n",
    "# And `max_episode_steps` determines the number of steps per episode before `truncation`,\n",
    "# here we set it to 1000 to be consistent with the based `Gymnasium/MuJoCo` environments,\n",
    "# but if you need something higher you can set it so.\n",
    "\n",
    "env = gym.make(\n",
    "    \"Ant-v5\",\n",
    "    xml_file=\"./unitree_go1/scene.xml\",\n",
    "    forward_reward_weight=0,\n",
    "    ctrl_cost_weight=0,\n",
    "    contact_cost_weight=0,\n",
    "    healthy_reward=0,\n",
    "    main_body=1,\n",
    "    healthy_z_range=(0, np.inf),\n",
    "    include_cfrc_ext_in_observation=True,\n",
    "    exclude_current_positions_from_observation=False,\n",
    "    reset_noise_scale=0.1,  # set to avoid policy overfitting\n",
    "    frame_skip=25,  # set dt=0.05\n",
    "    max_episode_steps=1000,  # kept at 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b07c592-a493-49e9-a112-d142eb76bde4",
   "metadata": {},
   "source": [
    "# Tweaking the Environment Termination Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "778c69c9-763e-4813-93ab-011b80adb24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The arguments of interest are `terminate_when_unhealthy` and `healthy_z_range`.\n",
    "\n",
    "# We want to set `healthy_z_range` to terminate the environment when the robot falls over,\n",
    "# or jumps really high, here we have to choose a value that is logical for the height of the robot,\n",
    "# for `Go1` we choose `(0.195, 0.75)`.\n",
    "# Note: `healthy_z_range` checks the absolute value of the height of the robot,\n",
    "# so if your scene contains different levels of elevation it should be set to `(-np.inf, np.inf)`\n",
    "\n",
    "# We could also set `terminate_when_unhealthy=False` to disable termination altogether,\n",
    "# which is not desirable in the case of `Go1`.\n",
    "\n",
    "env = gym.make(\n",
    "    \"Ant-v5\",\n",
    "    xml_file=\"./unitree_go1/scene.xml\",\n",
    "    forward_reward_weight=0,\n",
    "    ctrl_cost_weight=0,\n",
    "    contact_cost_weight=0,\n",
    "    healthy_reward=0,\n",
    "    main_body=1,\n",
    "    healthy_z_range=(\n",
    "        0.195,\n",
    "        0.75,\n",
    "    ),  # set to avoid sampling steps where the robot has fallen or jumped too high\n",
    "    include_cfrc_ext_in_observation=True,\n",
    "    exclude_current_positions_from_observation=False,\n",
    "    reset_noise_scale=0.1,\n",
    "    frame_skip=25,\n",
    "    max_episode_steps=1000,\n",
    ")\n",
    "\n",
    "# Note: If you need a different termination condition, you can write your own `TerminationWrapper`\n",
    "# (see the documentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3be2b61-e807-429e-9420-b7e8ddaf0cac",
   "metadata": {},
   "source": [
    "# Tweaking the Environment Reward Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d104af77-05cc-4158-89cb-973251dd4cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the arguments `forward_reward_weight`, `ctrl_cost_weight`, `contact_cost_weight` and `healthy_reward`\n",
    "# we have to pick values that make sense for our robot, you can use the default `MuJoCo/Ant`\n",
    "# parameters for references and tweak them if a change is needed for your environment.\n",
    "# In the case of `Go1` we only change the `ctrl_cost_weight` since it has a higher actuator force range.\n",
    "\n",
    "# For the argument `main_body` we have to choose which body part is the main body\n",
    "# (usually called something like \"torso\" or \"trunk\" in the model file) for the calculation\n",
    "# of the `forward_reward`, in the case of `Go1` it is the `\"trunk\"`\n",
    "# (Note: in most cases including this one, it can be left at the default value).\n",
    "\n",
    "env = gym.make(\n",
    "    \"Ant-v5\",\n",
    "    xml_file=\"./unitree_go1/scene.xml\",\n",
    "    forward_reward_weight=1,  # kept the same as the 'Ant' environment\n",
    "    ctrl_cost_weight=0.05,  # changed because of the stronger motors of `Go1`\n",
    "    contact_cost_weight=5e-4,  # kept the same as the 'Ant' environment\n",
    "    healthy_reward=1,  # kept the same as the 'Ant' environment\n",
    "    main_body=1,  # represents the \"trunk\" of the `Go1` robot\n",
    "    healthy_z_range=(0.195, 0.75),\n",
    "    include_cfrc_ext_in_observation=True,\n",
    "    exclude_current_positions_from_observation=False,\n",
    "    reset_noise_scale=0.1,\n",
    "    frame_skip=25,\n",
    "    max_episode_steps=1000,\n",
    ")\n",
    "\n",
    "# Note: If you need a different reward function, you can write your own `RewardWrapper`\n",
    "# (see the documentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b993748-8c91-4e12-b8ce-6a864fc4fcd4",
   "metadata": {},
   "source": [
    "# Tweaking the Environment Observation Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d5d0166-3516-4093-a43e-86dd7e9663fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here for `Go1` we have no particular reason to change them.\n",
    "\n",
    "env = gym.make(\n",
    "    \"Ant-v5\",\n",
    "    xml_file=\"./unitree_go1/scene.xml\",\n",
    "    forward_reward_weight=1,\n",
    "    ctrl_cost_weight=0.05,\n",
    "    contact_cost_weight=5e-4,\n",
    "    healthy_reward=1,\n",
    "    main_body=1,\n",
    "    healthy_z_range=(0.195, 0.75),\n",
    "    include_cfrc_ext_in_observation=True,  # kept the same as the 'Ant' environment\n",
    "    exclude_current_positions_from_observation=False,  # kept the same as the 'Ant' environment\n",
    "    reset_noise_scale=0.1,\n",
    "    frame_skip=25,\n",
    "    max_episode_steps=1000,\n",
    ")\n",
    "\n",
    "\n",
    "# Note: If you need additional observation elements (such as additional sensors),\n",
    "# you can write your own `ObservationWrapper` (see the documentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb0406e-64b7-4cf0-bc19-3a17f4be590f",
   "metadata": {},
   "source": [
    "# Train your Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb7b6c90-c9fb-4bd0-a0ff-935a625db42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run the final Go1 environment setup.\"\"\"\n",
    "\n",
    "    env = gym.make(\n",
    "        \"Ant-v5\",\n",
    "        xml_file=\"./unitree_go1/scene.xml\",\n",
    "        forward_reward_weight=1,\n",
    "        ctrl_cost_weight=0.05,\n",
    "        contact_cost_weight=5e-4,\n",
    "        healthy_reward=1,\n",
    "        main_body=1,\n",
    "        healthy_z_range=(0.195, 0.75),\n",
    "        include_cfrc_ext_in_observation=True,\n",
    "        exclude_current_positions_from_observation=False,\n",
    "        reset_noise_scale=0.1,\n",
    "        frame_skip=25,\n",
    "        max_episode_steps=1000,\n",
    "        render_mode=\"human\",  # Change to \"human\" to visualize\n",
    "    )\n",
    "\n",
    "    # Example of running the environment for a few steps\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    for _ in range(100):\n",
    "        action = env.action_space.sample()  # Replace with your agent's action\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            obs, info = env.reset()\n",
    "\n",
    "    env.close()\n",
    "    print(\"Environment tested successfully!\")\n",
    "\n",
    "    # 1. Set up your RL algorithm\n",
    "    # 2. Train the agent\n",
    "    # 3. Evaluate the agent's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25b2b89b-c287-42b5-8538-93a391e40936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment tested successfully!\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RL)",
   "language": "python",
   "name": "your_env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
