{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eabbd74d-6df7-4427-9b6e-71e724830516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import ActionWrapper, ObservationWrapper, RewardWrapper, Wrapper\n",
    "from gymnasium.spaces import Box, Discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ebb6cc-3f20-477f-9806-22f07bb64093",
   "metadata": {},
   "source": [
    "# Inheriting from gymnasium.ObservationWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d81d165-8958-41ad-bf1f-26d58db45926",
   "metadata": {},
   "source": [
    "Imagine you have a 2D navigation task where the environment returns dictionaries as observations with keys \"agent_position\" and \"target_position\". A common thing to do might be to throw away some degrees of freedom and only consider the position of the target relative to the agent, i.e. observation[\"target_position\"] - observation[\"agent_position\"]. For this, you could implement an observation wrapper like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0a0ae91-a3c3-43a3-bb91-e9f4713d9bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativePosition(ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = Box(shape=(2,), low=-np.inf, high=np.inf)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return obs[\"target\"] - obs[\"agent\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e3508b-106e-479a-aae4-3ac0c1387234",
   "metadata": {},
   "source": [
    "# Inheriting from gymnasium.ActionWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b21f285-111a-461d-8d74-e19525830e0e",
   "metadata": {},
   "source": [
    "Letâ€™s say you have an environment with action space of type gymnasium.spaces.Box, but you would only like to use a finite subset of actions. Then, you might want to implement the following wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de8fdfd8-497c-47c1-818e-5dcabc3f130c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(-1.0, 1.0, (2,), float32)\n",
      "Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "class DiscreteActions(ActionWrapper):\n",
    "    def __init__(self, env, disc_to_cont):\n",
    "        super().__init__(env)\n",
    "        self.disc_to_cont = disc_to_cont\n",
    "        self.action_space = Discrete(len(disc_to_cont))\n",
    "\n",
    "    def action(self, act):\n",
    "        return self.disc_to_cont[act]\n",
    "\n",
    "\n",
    "env = gym.make(\"LunarLanderContinuous-v3\")\n",
    "print(env.action_space)  # Box(-1.0, 1.0, (2,), float32)\n",
    "wrapped_env = DiscreteActions(\n",
    "    env, [np.array([1, 0]), np.array([-1, 0]), np.array([0, 1]), np.array([0, -1])]\n",
    ")\n",
    "print(wrapped_env.action_space)  # Discrete(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482e77ac-c295-4ff7-a138-e178874714e6",
   "metadata": {},
   "source": [
    "# Inheriting from gymnasium.RewardWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13af810-aec3-4260-b14b-c43a01ff9172",
   "metadata": {},
   "source": [
    "Sometimes (especially when we do not have control over the reward because it is intrinsic), we want to clip the reward to a range to gain some numerical stability. To do that, we could, for instance, implement the following wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "501d445c-3501-4ec0-907d-8d29f9d8527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import SupportsFloat\n",
    "\n",
    "\n",
    "class ClipReward(RewardWrapper):\n",
    "    def __init__(self, env, min_reward, max_reward):\n",
    "        super().__init__(env)\n",
    "        self.min_reward = min_reward\n",
    "        self.max_reward = max_reward\n",
    "\n",
    "    def reward(self, r: SupportsFloat) -> SupportsFloat:\n",
    "        return np.clip(r, self.min_reward, self.max_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8f0f9a-0d7b-43de-9b9f-e4effcd1818b",
   "metadata": {},
   "source": [
    "# Inheriting from gymnasium.Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61fd22b-7cfc-4e91-aaac-e5beecff3e6f",
   "metadata": {},
   "source": [
    "Most MuJoCo environments return a reward that consists of different terms: For instance, there might be a term that rewards the agent for completing the task and one term that penalizes large actions (i.e. energy usage). Usually, you can pass weight parameters for those terms during initialization of the environment. However, Reacher does not allow you to do this! Nevertheless, all individual terms of the reward are returned in info, so let us build a wrapper for Reacher that allows us to weight those terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fb16a94-d846-48f8-b725-00bdf9111435",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReacherRewardWrapper(Wrapper):\n",
    "    def __init__(self, env, reward_dist_weight, reward_ctrl_weight):\n",
    "        super().__init__(env)\n",
    "        self.reward_dist_weight = reward_dist_weight\n",
    "        self.reward_ctrl_weight = reward_ctrl_weight\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, _, terminated, truncated, info = self.env.step(action)\n",
    "        reward = (\n",
    "            self.reward_dist_weight * info[\"reward_dist\"]\n",
    "            + self.reward_ctrl_weight * info[\"reward_ctrl\"]\n",
    "        )\n",
    "        return obs, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73c2928-85ad-40d0-bae5-8f89d65d633f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RL)",
   "language": "python",
   "name": "your_env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
